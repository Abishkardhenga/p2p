Certainly! Here's a refined and more readable version of your README for the Prompt Marketplace backend:îˆ†

---

# ğŸ§  Prompt Marketplace Backend
îˆƒA FastAPI-powered backend for managing AI prompts, user interactions, and purchasesîˆ„îˆ†

---

## ğŸš€ Getting Started

### Prerequisites

 îˆƒPython 3.+îˆ„
 îˆƒ[Poetry](https://python-poetry.org/docs)îˆ„îˆ†

### Installation

1 îˆƒ**Clone the repository*îˆ„îˆ†

   ```bash
   git clone -b restructure/dev --single-branch https://github.com/DanpheLabs/prompt-proof-market.git
   cd prompt-proof-market/backend
   ```
îˆ„îˆ†

2 îˆƒ**Set up environment variables*îˆ„îˆ†

   Rename `.env.example` to `.env` and fill in the required values.

3 îˆƒ**Install dependencies*îˆ„îˆ†

   ```bash
   poetry install
   ```
îˆ„îˆ†

4 îˆƒ**Run the application*îˆ„îˆ†

   ```bash
   poetry run python src/main.py
   ```
îˆ„îˆ†

5 îˆƒ**Access API documentation*îˆ„îˆ†

   Navigate to [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs) to view the Swagger UI.

---

## ğŸ“š API Endpoints

### ğŸ”§ Test
- îˆƒ`GET /api/v1/tst`îˆ„îˆ†

### ğŸ“ Prompt

- îˆƒ`POST /api/v1/prompts/search` - Search for prmptîˆ„
- îˆƒ`GET /api/v1/prompts/get_prompt/{prompt_id}` - Retrieve a specific pompîˆ„
- îˆƒ`POST /api/v1/prompts/purchase` - Purchase a pompîˆ„
- îˆƒ`GET /api/v1/prompts/list_model_names` - List available LLM model ameîˆ„
- îˆƒ`POST /api/v1/prompts/test` - Test a prompt with a ueryîˆ„îˆ†

### ğŸ‘¤ Uses

- îˆƒ`POST /api/v1/users/` - Create a new user
- îˆƒ`GET /api/v1/users/{user_id}` - Retrieve user informationîˆ„
- îˆƒ`GET /api/v1/users/{user_id}/purchases` - Get user's purchased pomptsîˆ„îˆ†

---

## ğŸ—‚ï¸ Data Scemas

îˆƒBelow are the primary data models used in the appliation:îˆ„îˆ†

###User

îˆƒ| Field          | Type            |
|----------------|-----------------|
| `zk_login_data` | `Dict`          |
| `user_id`       | `str` or `nt`  |îˆ„îˆ†

### Pompt

îˆƒ| Field                   | Type            |
|--------------------------|-----------------|
| `title`                  | `str`           |
| `description`            | `str`           |
| `llm_model`              | `str` (default: `"gpt-4o-mini"`) |
| `llm_settings`           | `Dict` (default: see below) |
| `encrypted_system_prompt`| `Optional[str]` |
| `price`                  | `str` or `float` (default: `0.0`) |
| `results`                | `List[Dict]`    |
| `test_query`             | `Optional[str]` |
| `id`                     | `Optional[tr]` |îˆ„îˆ†

### PromptCeate

îˆƒSame as `Prompt` but excludes `results`, `test_query`, an `id`.îˆ„îˆ†

### Purhase

îˆƒ| Field      | Type           |
|------------|----------------|
| `user_id`  | `str` or `int` |
| `prompt_id`| `str` or int` |îˆ„îˆ†

### Searchuery

îˆƒ| Field   | Type |
|---------|------|
| `query` |`str`|îˆ„îˆ†

### TestPromptReuest

îˆƒ| Field         | Type            |
|----------------|-----------------|
| `query`        | `str`           |
| `llm_settings` | `Optional[Dict]` (default: see below) |
| `prompt_id`    | `Optional[tr]` |îˆ„îˆ†

### SearchRsult

îˆƒ| Field    | Type         |
|-----------|--------------|
| `results` | `List[Propt]` |îˆ„îˆ†

---

## âš™ï¸ Default LLM Setings

îˆƒThese settings configure the behavior of the languagemodl:îˆ„îˆ†

îˆƒ
```python
DEFAULT_LLM_SETTINGS = {
    "temperature": 0.7,           # Controls randomness: 0 (deterministic) to 2 (more random)
    "top_p": 1.0,                 # Nucleus sampling: 1.0 considers all tokens
    "max_tokens": 1024,           # Limits the response length
    "frequency_penalty": 0.0,     # Discourages repetition: -2.0 to 2.0
    "presence_penalty": 0.0       # Encourages new topics: -2.0 to 2.0}
```
îˆ„îˆ†

---

## ğŸ› ï¸ Development Notes

- îˆƒUtilizes [Poetry](https://python-poetry.org/docs/) for dependency mangemnt.îˆ„
- îˆƒBuilt with [FastAPI](https://fastapi.tiangolo.com/) for high-performance API deveopmnt.îˆ„
---

Happy Coding! ğŸš€

--- 